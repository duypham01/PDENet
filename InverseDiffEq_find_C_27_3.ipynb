{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InverseDiffEq_find_C_27/3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPztuaHaD1igadjZWGgUEuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duypham01/PDENet/blob/main/InverseDiffEq_find_C_27_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_98MYZKOz9j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import json\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNTm9oC2ZvMK",
        "outputId": "4c5ae781-737d-4f8b-8aae-a5a6a5a00fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.718281828459045"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def u_exact(x,t):\n",
        "    return np.exp(-np.pi**2*t)*np.sin(np.pi*x)\n",
        "def generateObservation(h, x_arr, delta_t, t_0, n_t):\n",
        "    x = []\n",
        "    tmp = x_arr[0]\n",
        "    while tmp <= x_arr[1]:\n",
        "        x.append(tmp)\n",
        "        tmp = tmp + h\n",
        "    U = []\n",
        "    t = []\n",
        "    for i in range(n_t):\n",
        "        t_tmp = t_0 + i*delta_t\n",
        "        t.append(t_tmp)\n",
        "        tmp = []\n",
        "        for el in x:\n",
        "            tmp.append(u_exact(el, t_tmp))\n",
        "        U.append(tmp)\n",
        "    return U, x, t"
      ],
      "metadata": {
        "id": "Y-PkMcSZO9Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, x, t = generateObservation(0.002, [-1,1], 0.001, 0.1, 2)"
      ],
      "metadata": {
        "id": "DqppzA5ja1bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "cn_sxTj5bLoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(U[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqj4x98ubNVh",
        "outputId": "c5d22e4d-1cd8-4d3a-82ef-aa5a27aa4ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD NETWORK\n",
        "\n",
        "# dimension of input and output\n",
        "in_dim = 1\n",
        "out_dim = 1\n",
        "\n",
        "# number of neurons on each layer\n",
        "nn = [16, 16, 16, 16]\n",
        "\n",
        "# input layer\n",
        "inputs = keras.Input(shape=(in_dim,), name='points')\n",
        "\n",
        "# hidden layers\n",
        "hidden = keras.layers.Dense(nn[0], activation='tanh', name='hidden_1')(inputs)\n",
        "for i in range(len(nn)-1):\n",
        "    hidden = keras.layers.Dense(nn[i+1], activation='tanh', name='hidden_' + str(i+2))(hidden)\n",
        "\n",
        "# output layer\n",
        "outputs = keras.layers.Dense(out_dim, activation='linear', name=\"u\")(hidden)\n",
        "\n",
        "# create network\n",
        "PDEmodel = keras.Model(inputs=inputs, outputs=outputs, name='heat')\n",
        "\n",
        "# show network details\n",
        "PDEmodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVqNHSMNcEm9",
        "outputId": "8940cd75-a952-42c4-af43-32dfbb1d9f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"heat\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " points (InputLayer)         [(None, 1)]               0         \n",
            "                                                                 \n",
            " hidden_1 (Dense)            (None, 16)                32        \n",
            "                                                                 \n",
            " hidden_2 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " hidden_3 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " hidden_4 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " u (Dense)                   (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 865\n",
            "Trainable params: 865\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fun_f1(x,t):\n",
        "    return np.pi**2*(1+np.exp(-(x-0.5)**2))*np.exp(-np.pi**2*t)*np.sin(np.pi*x)-np.pi**2*np.exp(-np.pi**2*t)*np.sin(np.pi*x)\n",
        "\n",
        "def fun_f(x,t):\n",
        "    return np.pi**2*(1+tf.math.exp(-(x-0.5)**2))*tf.math.exp(-np.pi**2*t)*tf.math.sin(np.pi*x)-np.pi**2*tf.math.exp(-np.pi**2*t)*tf.math.sin(np.pi*x)\n",
        "# return 1D weights\n",
        "def get_weights():\n",
        "    w = []\n",
        "    for layer in PDEmodel.layers[1:]:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights = weights_biases[0].flatten()\n",
        "        biases = weights_biases[1]\n",
        "        w.extend(weights)\n",
        "        w.extend(biases)\n",
        "    return w\n",
        "\n",
        "# get size of weights in each model's layer\n",
        "sizes_w = []\n",
        "sizes_b = []\n",
        "for layer in PDEmodel.layers[1:]:\n",
        "    weights_biases = layer.get_weights()\n",
        "    sizes_w.append(weights_biases[0].flatten().size)\n",
        "    sizes_b.append(weights_biases[1].size)\n",
        "\n",
        "# convert 1D weights to multi dimension weights in each model's layer\n",
        "def set_weights(w):\n",
        "    for i, layer in enumerate(PDEmodel.layers[1:]):\n",
        "        start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
        "        end_weights = sum(sizes_w[:i+1]) + sum(sizes_b[:i])\n",
        "        weights = w[start_weights:end_weights]\n",
        "        w_div = int(sizes_w[i] / sizes_b[i])\n",
        "        weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
        "        biases = w[end_weights:end_weights + sizes_b[i]]\n",
        "        weights_biases = [weights, biases]\n",
        "        layer.set_weights(weights_biases)\n",
        "def train(U, x, t, delta_t, h, epochs, optimizer):\n",
        "    losses_hist = []\n",
        "    # x_in = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "\n",
        "    if (optimizer == 'adam'):\n",
        "        optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999)\n",
        "        for epoch in range(epochs):\n",
        "            start_epoch = time.time()\n",
        "            loss = [0]\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss = 0\n",
        "                for i in range(len(t)-1):\n",
        "                    for j in range(1,len(x)-1):\n",
        "                        x_in = tf.reshape(tf.convert_to_tensor(x[j], dtype=tf.float32), [1])\n",
        "                        t_in1 = tf.reshape(tf.convert_to_tensor(t[i+1], dtype=tf.float32), [1])\n",
        "                        t_in = tf.reshape(tf.convert_to_tensor(t[i], dtype=tf.float32), [1])\n",
        "                        loss = loss + tf.math.square(1/delta_t*(U[i+1][j]-U[i][j]) - PDEmodel(x_in)[0][0]/(2*h**2)*(U[i+1][j+1]+U[i+1][j-1]-2*U[i+1][j]) \\\n",
        "                        - PDEmodel(x_in)[0][0]/(2*h**2)*(U[i][j+1]+U[i][j-1]-2*U[i][j])-0.5*(fun_f(x_in,t_in)[0]+fun_f(x_in,t_in1)[0]))\n",
        "            grads = tape.gradient(loss, PDEmodel.weights)\n",
        "            optimizer.apply_gradients(zip(grads, PDEmodel.weights))\n",
        "            print(\"[%4s] loss = %12.8f \\t %4.3fs\" % (epoch, loss, time.time() - start_epoch))\n",
        "            losses_hist.append(loss.numpy())\n",
        "    if (optimizer == 'l-bfgs'):\n",
        "        def function_factory(x):\n",
        "            def loss_grad(weights):\n",
        "                start_epoch = time.time()\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss = 0\n",
        "                    for i in range(len(t)-1):\n",
        "                        for j in range(1,len(x)-1):\n",
        "                            x_in = tf.reshape(tf.convert_to_tensor(x[j], dtype=tf.float32), [1])\n",
        "                            t_in1 = tf.reshape(tf.convert_to_tensor(t[i+1], dtype=tf.float32), [1])\n",
        "                            t_in = tf.reshape(tf.convert_to_tensor(t[i], dtype=tf.float32), [1])\n",
        "                            loss = loss + tf.math.square(1/delta_t*(U[i+1][j]-U[i][j]) - PDEmodel(x_in)[0][0]/(2*h**2)*(U[i+1][j+1]+U[i+1][j-1]-2*U[i+1][j]) \\\n",
        "                            - PDEmodel(x_in)[0][0]/(2*h**2)*(U[i][j+1]+U[i][j-1]-2*U[i][j])-0.5*(fun_f(x_in,t_in)[0]+fun_f(x_in,t_in1)[0]))\n",
        "                    print(\"loss = %12.5f \\t %4.3fs\" % (loss, time.time() - start_epoch))\n",
        "                    losses_hist.append(loss.numpy())\n",
        "                grad = tape.gradient(loss, PDEmodel.weights)\n",
        "                grad_1D = []\n",
        "                for g in grad:\n",
        "                    grad_1D.append(tf.reshape(g, [-1]))\n",
        "                grad_1D = tf.concat(grad_1D, 0)\n",
        "                return loss, grad_1D\n",
        "            return loss_grad\n",
        "        # update paremeters\n",
        "\n",
        "        func = function_factory(x)\n",
        "        # add loss_batch to loss\n",
        "        tfp.optimizer.lbfgs_minimize(func,\n",
        "            tf.convert_to_tensor(get_weights(), dtype=tf.float32),\n",
        "            max_iterations=epochs)"
      ],
      "metadata": {
        "id": "AHSfxStFc_O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(U, x, t, 0.001, 0.002, 500, 'adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySGjINuUgAx0",
        "outputId": "044937d7-784d-437b-c41c-7fe061877835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0] loss =   0.18728527 \t 14.176s\n",
            "[   1] loss =   0.21572536 \t 9.909s\n",
            "[   2] loss =  18.39556122 \t 10.371s\n",
            "[   3] loss =   0.71166837 \t 10.791s\n",
            "[   4] loss =   9.24794102 \t 10.033s\n",
            "[   5] loss =  11.88600636 \t 10.064s\n",
            "[   6] loss =   4.72875500 \t 10.125s\n",
            "[   7] loss =   1.04070866 \t 10.241s\n",
            "[   8] loss =   3.53366470 \t 10.310s\n",
            "[   9] loss =   6.22518253 \t 9.932s\n",
            "[  10] loss =   5.27268124 \t 10.155s\n",
            "[  11] loss =   2.45706415 \t 10.201s\n",
            "[  12] loss =   0.87771755 \t 10.292s\n",
            "[  13] loss =   1.56921601 \t 10.075s\n",
            "[  14] loss =   3.16238499 \t 10.242s\n",
            "[  15] loss =   3.49326396 \t 10.108s\n",
            "[  16] loss =   2.13319635 \t 10.097s\n",
            "[  17] loss =   0.58588505 \t 10.242s\n",
            "[  18] loss =   0.43978316 \t 10.179s\n",
            "[  19] loss =   1.58184004 \t 10.071s\n",
            "[  20] loss =   2.32227612 \t 10.459s\n",
            "[  21] loss =   1.68421853 \t 10.112s\n",
            "[  22] loss =   0.53562105 \t 10.080s\n",
            "[  23] loss =   0.20062898 \t 10.378s\n",
            "[  24] loss =   0.80747819 \t 10.104s\n",
            "[  25] loss =   1.35934162 \t 10.242s\n",
            "[  26] loss =   1.17628610 \t 10.201s\n",
            "[  27] loss =   0.58195978 \t 10.579s\n",
            "[  28] loss =   0.24882782 \t 10.319s\n",
            "[  29] loss =   0.40799001 \t 10.083s\n",
            "[  30] loss =   0.74328059 \t 10.295s\n",
            "[  31] loss =   0.82582396 \t 10.268s\n",
            "[  32] loss =   0.55516988 \t 10.185s\n",
            "[  33] loss =   0.23711972 \t 10.319s\n",
            "[  34] loss =   0.22749196 \t 10.046s\n",
            "[  35] loss =   0.47933224 \t 10.108s\n",
            "[  36] loss =   0.60258621 \t 10.089s\n",
            "[  37] loss =   0.41961756 \t 10.327s\n",
            "[  38] loss =   0.19414844 \t 10.125s\n",
            "[  39] loss =   0.20555225 \t 10.301s\n",
            "[  40] loss =   0.36495903 \t 10.115s\n",
            "[  41] loss =   0.41517246 \t 10.183s\n",
            "[  42] loss =   0.30586985 \t 10.163s\n",
            "[  43] loss =   0.19423138 \t 10.234s\n",
            "[  44] loss =   0.19821766 \t 9.989s\n",
            "[  45] loss =   0.27749792 \t 10.324s\n",
            "[  46] loss =   0.31229448 \t 10.276s\n",
            "[  47] loss =   0.25044173 \t 10.171s\n",
            "[  48] loss =   0.17283210 \t 10.189s\n",
            "[  49] loss =   0.18204798 \t 10.200s\n",
            "[  50] loss =   0.24703620 \t 10.239s\n",
            "[  51] loss =   0.25470141 \t 10.481s\n",
            "[  52] loss =   0.19476087 \t 10.024s\n",
            "[  53] loss =   0.16175105 \t 10.099s\n",
            "[  54] loss =   0.19076116 \t 10.079s\n",
            "[  55] loss =   0.21964565 \t 10.058s\n",
            "[  56] loss =   0.20439173 \t 10.232s\n",
            "[  57] loss =   0.17249136 \t 10.124s\n",
            "[  58] loss =   0.16562521 \t 10.167s\n",
            "[  59] loss =   0.18506785 \t 10.079s\n",
            "[  60] loss =   0.19686003 \t 10.349s\n",
            "[  61] loss =   0.18034376 \t 10.024s\n",
            "[  62] loss =   0.16036780 \t 10.572s\n",
            "[  63] loss =   0.16686127 \t 10.127s\n",
            "[  64] loss =   0.18314117 \t 10.056s\n",
            "[  65] loss =   0.17842597 \t 9.966s\n",
            "[  66] loss =   0.16236353 \t 10.234s\n",
            "[  67] loss =   0.16090477 \t 10.157s\n",
            "[  68] loss =   0.17061503 \t 10.376s\n",
            "[  69] loss =   0.17264295 \t 10.217s\n",
            "[  70] loss =   0.16472663 \t 10.087s\n",
            "[  71] loss =   0.15907086 \t 10.094s\n",
            "[  72] loss =   0.16261208 \t 10.765s\n",
            "[  73] loss =   0.16771629 \t 10.407s\n",
            "[  74] loss =   0.16446389 \t 10.375s\n",
            "[  75] loss =   0.15809442 \t 10.086s\n",
            "[  76] loss =   0.15925109 \t 10.031s\n",
            "[  77] loss =   0.16377942 \t 10.233s\n",
            "[  78] loss =   0.16228455 \t 10.129s\n",
            "[  79] loss =   0.15796560 \t 10.062s\n",
            "[  80] loss =   0.15793078 \t 10.174s\n",
            "[  81] loss =   0.16040844 \t 10.179s\n",
            "[  82] loss =   0.16038367 \t 10.264s\n",
            "[  83] loss =   0.15788838 \t 10.077s\n",
            "[  84] loss =   0.15677385 \t 10.149s\n",
            "[  85] loss =   0.15834141 \t 10.067s\n",
            "[  86] loss =   0.15903550 \t 10.150s\n",
            "[  87] loss =   0.15711501 \t 10.166s\n",
            "[  88] loss =   0.15606074 \t 10.165s\n",
            "[  89] loss =   0.15724322 \t 10.195s\n",
            "[  90] loss =   0.15760703 \t 10.143s\n",
            "[  91] loss =   0.15634705 \t 10.298s\n",
            "[  92] loss =   0.15570401 \t 10.060s\n",
            "[  93] loss =   0.15623383 \t 10.276s\n",
            "[  94] loss =   0.15649362 \t 10.159s\n",
            "[  95] loss =   0.15578523 \t 10.159s\n",
            "[  96] loss =   0.15515256 \t 10.391s\n",
            "[  97] loss =   0.15547536 \t 10.305s\n",
            "[  98] loss =   0.15571822 \t 10.449s\n",
            "[  99] loss =   0.15508614 \t 10.293s\n",
            "[ 100] loss =   0.15466268 \t 9.927s\n",
            "[ 101] loss =   0.15492176 \t 9.992s\n",
            "[ 102] loss =   0.15493239 \t 10.230s\n",
            "[ 103] loss =   0.15448813 \t 9.851s\n",
            "[ 104] loss =   0.15423271 \t 10.255s\n",
            "[ 105] loss =   0.15431599 \t 10.305s\n",
            "[ 106] loss =   0.15429106 \t 10.240s\n",
            "[ 107] loss =   0.15395254 \t 10.436s\n",
            "[ 108] loss =   0.15372761 \t 9.843s\n",
            "[ 109] loss =   0.15379557 \t 10.036s\n",
            "[ 110] loss =   0.15370099 \t 10.439s\n",
            "[ 111] loss =   0.15339786 \t 10.207s\n",
            "[ 112] loss =   0.15327488 \t 10.323s\n",
            "[ 113] loss =   0.15326953 \t 10.079s\n",
            "[ 114] loss =   0.15312147 \t 10.215s\n",
            "[ 115] loss =   0.15290384 \t 10.197s\n",
            "[ 116] loss =   0.15279102 \t 10.267s\n",
            "[ 117] loss =   0.15274246 \t 10.214s\n",
            "[ 118] loss =   0.15259188 \t 10.372s\n",
            "[ 119] loss =   0.15239578 \t 10.036s\n",
            "[ 120] loss =   0.15230781 \t 10.032s\n",
            "[ 121] loss =   0.15222816 \t 10.140s\n",
            "[ 122] loss =   0.15205686 \t 10.116s\n",
            "[ 123] loss =   0.15190783 \t 10.393s\n",
            "[ 124] loss =   0.15181741 \t 10.364s\n",
            "[ 125] loss =   0.15170170 \t 10.281s\n",
            "[ 126] loss =   0.15154761 \t 10.301s\n",
            "[ 127] loss =   0.15141337 \t 10.279s\n",
            "[ 128] loss =   0.15131445 \t 10.247s\n",
            "[ 129] loss =   0.15118630 \t 10.218s\n",
            "[ 130] loss =   0.15103392 \t 10.207s\n",
            "[ 131] loss =   0.15091681 \t 10.075s\n",
            "[ 132] loss =   0.15080482 \t 10.095s\n",
            "[ 133] loss =   0.15066503 \t 10.176s\n",
            "[ 134] loss =   0.15052879 \t 10.055s\n",
            "[ 135] loss =   0.15041128 \t 10.021s\n",
            "[ 136] loss =   0.15028822 \t 10.034s\n",
            "[ 137] loss =   0.15014978 \t 10.282s\n",
            "[ 138] loss =   0.15001926 \t 10.172s\n",
            "[ 139] loss =   0.14989939 \t 10.250s\n",
            "[ 140] loss =   0.14976837 \t 10.227s\n",
            "[ 141] loss =   0.14963143 \t 10.078s\n",
            "[ 142] loss =   0.14950620 \t 10.078s\n",
            "[ 143] loss =   0.14937942 \t 10.006s\n",
            "[ 144] loss =   0.14924373 \t 10.181s\n",
            "[ 145] loss =   0.14911310 \t 10.167s\n",
            "[ 146] loss =   0.14898549 \t 10.057s\n",
            "[ 147] loss =   0.14885467 \t 10.204s\n",
            "[ 148] loss =   0.14871950 \t 10.060s\n",
            "[ 149] loss =   0.14858809 \t 10.144s\n",
            "[ 150] loss =   0.14845853 \t 10.120s\n",
            "[ 151] loss =   0.14832467 \t 10.427s\n",
            "[ 152] loss =   0.14819071 \t 10.095s\n",
            "[ 153] loss =   0.14805987 \t 9.989s\n",
            "[ 154] loss =   0.14792719 \t 10.024s\n",
            "[ 155] loss =   0.14779201 \t 10.215s\n",
            "[ 156] loss =   0.14765981 \t 10.170s\n",
            "[ 157] loss =   0.14752644 \t 10.292s\n",
            "[ 158] loss =   0.14739092 \t 10.088s\n",
            "[ 159] loss =   0.14725618 \t 10.105s\n",
            "[ 160] loss =   0.14712293 \t 10.211s\n",
            "[ 161] loss =   0.14698723 \t 10.045s\n",
            "[ 162] loss =   0.14685133 \t 10.211s\n",
            "[ 163] loss =   0.14671546 \t 10.084s\n",
            "[ 164] loss =   0.14658016 \t 9.996s\n",
            "[ 165] loss =   0.14644398 \t 10.145s\n",
            "[ 166] loss =   0.14630744 \t 10.087s\n",
            "[ 167] loss =   0.14617185 \t 9.950s\n",
            "[ 168] loss =   0.14603521 \t 10.197s\n",
            "[ 169] loss =   0.14589658 \t 10.147s\n",
            "[ 170] loss =   0.14576018 \t 10.024s\n",
            "[ 171] loss =   0.14562225 \t 10.186s\n",
            "[ 172] loss =   0.14548494 \t 10.048s\n",
            "[ 173] loss =   0.14534667 \t 9.979s\n",
            "[ 174] loss =   0.14520837 \t 10.068s\n",
            "[ 175] loss =   0.14506954 \t 10.338s\n",
            "[ 176] loss =   0.14493084 \t 10.252s\n",
            "[ 177] loss =   0.14479199 \t 10.192s\n",
            "[ 178] loss =   0.14465314 \t 10.031s\n",
            "[ 179] loss =   0.14451362 \t 9.999s\n",
            "[ 180] loss =   0.14437425 \t 10.318s\n",
            "[ 181] loss =   0.14423397 \t 10.056s\n",
            "[ 182] loss =   0.14409332 \t 10.052s\n",
            "[ 183] loss =   0.14395267 \t 10.024s\n",
            "[ 184] loss =   0.14381310 \t 10.064s\n",
            "[ 185] loss =   0.14367141 \t 10.155s\n",
            "[ 186] loss =   0.14353094 \t 10.168s\n",
            "[ 187] loss =   0.14338921 \t 10.429s\n",
            "[ 188] loss =   0.14324847 \t 10.082s\n",
            "[ 189] loss =   0.14310585 \t 10.403s\n",
            "[ 190] loss =   0.14296378 \t 10.078s\n",
            "[ 191] loss =   0.14282167 \t 9.992s\n",
            "[ 192] loss =   0.14267978 \t 10.356s\n",
            "[ 193] loss =   0.14253800 \t 10.067s\n",
            "[ 194] loss =   0.14239551 \t 10.150s\n",
            "[ 195] loss =   0.14225206 \t 10.654s\n",
            "[ 196] loss =   0.14210807 \t 9.857s\n",
            "[ 197] loss =   0.14196520 \t 10.169s\n",
            "[ 198] loss =   0.14182192 \t 10.318s\n",
            "[ 199] loss =   0.14167865 \t 10.007s\n",
            "[ 200] loss =   0.14153469 \t 10.267s\n",
            "[ 201] loss =   0.14139041 \t 10.278s\n",
            "[ 202] loss =   0.14124601 \t 10.058s\n",
            "[ 203] loss =   0.14110100 \t 10.172s\n",
            "[ 204] loss =   0.14095670 \t 10.196s\n",
            "[ 205] loss =   0.14081290 \t 10.195s\n",
            "[ 206] loss =   0.14066672 \t 9.999s\n",
            "[ 207] loss =   0.14052233 \t 10.082s\n",
            "[ 208] loss =   0.14037630 \t 10.394s\n",
            "[ 209] loss =   0.14023080 \t 10.205s\n",
            "[ 210] loss =   0.14008494 \t 10.351s\n",
            "[ 211] loss =   0.13993910 \t 10.486s\n",
            "[ 212] loss =   0.13979305 \t 9.973s\n",
            "[ 213] loss =   0.13964731 \t 10.102s\n",
            "[ 214] loss =   0.13950022 \t 10.297s\n",
            "[ 215] loss =   0.13935427 \t 9.842s\n",
            "[ 216] loss =   0.13920777 \t 10.065s\n",
            "[ 217] loss =   0.13906004 \t 10.200s\n",
            "[ 218] loss =   0.13891383 \t 10.221s\n",
            "[ 219] loss =   0.13876644 \t 10.226s\n",
            "[ 220] loss =   0.13861930 \t 10.203s\n",
            "[ 221] loss =   0.13847153 \t 10.291s\n",
            "[ 222] loss =   0.13832380 \t 10.153s\n",
            "[ 223] loss =   0.13817593 \t 10.106s\n",
            "[ 224] loss =   0.13802773 \t 10.098s\n",
            "[ 225] loss =   0.13788016 \t 10.116s\n",
            "[ 226] loss =   0.13773212 \t 10.277s\n",
            "[ 227] loss =   0.13758354 \t 10.253s\n",
            "[ 228] loss =   0.13743466 \t 10.278s\n",
            "[ 229] loss =   0.13728641 \t 10.013s\n",
            "[ 230] loss =   0.13713802 \t 10.010s\n",
            "[ 231] loss =   0.13698834 \t 10.066s\n",
            "[ 232] loss =   0.13683915 \t 10.248s\n",
            "[ 233] loss =   0.13669011 \t 10.143s\n",
            "[ 234] loss =   0.13654055 \t 10.419s\n",
            "[ 235] loss =   0.13639116 \t 10.100s\n",
            "[ 236] loss =   0.13624164 \t 10.207s\n",
            "[ 237] loss =   0.13609122 \t 10.134s\n",
            "[ 238] loss =   0.13594177 \t 10.124s\n",
            "[ 239] loss =   0.13579233 \t 10.107s\n",
            "[ 240] loss =   0.13564126 \t 10.193s\n",
            "[ 241] loss =   0.13549107 \t 9.973s\n",
            "[ 242] loss =   0.13534096 \t 10.019s\n",
            "[ 243] loss =   0.13519029 \t 10.078s\n",
            "[ 244] loss =   0.13503940 \t 9.997s\n",
            "[ 245] loss =   0.13488878 \t 9.978s\n",
            "[ 246] loss =   0.13473798 \t 10.009s\n",
            "[ 247] loss =   0.13458689 \t 10.187s\n",
            "[ 248] loss =   0.13443545 \t 10.035s\n",
            "[ 249] loss =   0.13428448 \t 10.089s\n",
            "[ 250] loss =   0.13413346 \t 10.362s\n",
            "[ 251] loss =   0.13398115 \t 10.446s\n",
            "[ 252] loss =   0.13382995 \t 10.080s\n",
            "[ 253] loss =   0.13367866 \t 10.035s\n",
            "[ 254] loss =   0.13352674 \t 10.117s\n",
            "[ 255] loss =   0.13337520 \t 10.290s\n",
            "[ 256] loss =   0.13322225 \t 9.972s\n",
            "[ 257] loss =   0.13307047 \t 10.097s\n",
            "[ 258] loss =   0.13291864 \t 10.220s\n",
            "[ 259] loss =   0.13276632 \t 10.049s\n",
            "[ 260] loss =   0.13261349 \t 10.210s\n",
            "[ 261] loss =   0.13246186 \t 10.221s\n",
            "[ 262] loss =   0.13230877 \t 9.973s\n",
            "[ 263] loss =   0.13215587 \t 10.256s\n",
            "[ 264] loss =   0.13200268 \t 10.165s\n",
            "[ 265] loss =   0.13185020 \t 9.994s\n",
            "[ 266] loss =   0.13169695 \t 10.228s\n",
            "[ 267] loss =   0.13154399 \t 10.119s\n",
            "[ 268] loss =   0.13139078 \t 10.253s\n",
            "[ 269] loss =   0.13123728 \t 10.176s\n",
            "[ 270] loss =   0.13108410 \t 10.343s\n",
            "[ 271] loss =   0.13093086 \t 10.049s\n",
            "[ 272] loss =   0.13077672 \t 10.185s\n",
            "[ 273] loss =   0.13062283 \t 10.210s\n",
            "[ 274] loss =   0.13047001 \t 10.102s\n",
            "[ 275] loss =   0.13031587 \t 10.296s\n",
            "[ 276] loss =   0.13016182 \t 10.123s\n",
            "[ 277] loss =   0.13000795 \t 10.237s\n",
            "[ 278] loss =   0.12985393 \t 10.085s\n",
            "[ 279] loss =   0.12969959 \t 10.209s\n",
            "[ 280] loss =   0.12954602 \t 10.127s\n",
            "[ 281] loss =   0.12939188 \t 10.025s\n",
            "[ 282] loss =   0.12923734 \t 10.145s\n",
            "[ 283] loss =   0.12908256 \t 10.052s\n",
            "[ 284] loss =   0.12892829 \t 10.252s\n",
            "[ 285] loss =   0.12877308 \t 10.329s\n",
            "[ 286] loss =   0.12861957 \t 10.021s\n",
            "[ 287] loss =   0.12846385 \t 10.098s\n",
            "[ 288] loss =   0.12830931 \t 10.042s\n",
            "[ 289] loss =   0.12815492 \t 10.090s\n",
            "[ 290] loss =   0.12799932 \t 10.089s\n",
            "[ 291] loss =   0.12784445 \t 10.075s\n",
            "[ 292] loss =   0.12769011 \t 10.140s\n",
            "[ 293] loss =   0.12753445 \t 10.387s\n",
            "[ 294] loss =   0.12737876 \t 10.129s\n",
            "[ 295] loss =   0.12722382 \t 10.032s\n",
            "[ 296] loss =   0.12706853 \t 10.209s\n",
            "[ 297] loss =   0.12691358 \t 10.044s\n",
            "[ 298] loss =   0.12675767 \t 10.067s\n",
            "[ 299] loss =   0.12660176 \t 10.071s\n",
            "[ 300] loss =   0.12644574 \t 10.260s\n",
            "[ 301] loss =   0.12629129 \t 10.097s\n",
            "[ 302] loss =   0.12613563 \t 10.108s\n",
            "[ 303] loss =   0.12597986 \t 10.190s\n",
            "[ 304] loss =   0.12582335 \t 10.125s\n",
            "[ 305] loss =   0.12566811 \t 10.146s\n",
            "[ 306] loss =   0.12551209 \t 10.126s\n",
            "[ 307] loss =   0.12535673 \t 10.345s\n",
            "[ 308] loss =   0.12520070 \t 10.148s\n",
            "[ 309] loss =   0.12504393 \t 10.202s\n",
            "[ 310] loss =   0.12488814 \t 10.026s\n",
            "[ 311] loss =   0.12473256 \t 10.225s\n",
            "[ 312] loss =   0.12457673 \t 10.049s\n",
            "[ 313] loss =   0.12441996 \t 10.322s\n",
            "[ 314] loss =   0.12426389 \t 10.139s\n",
            "[ 315] loss =   0.12410788 \t 10.082s\n",
            "[ 316] loss =   0.12395180 \t 10.464s\n",
            "[ 317] loss =   0.12379429 \t 10.156s\n",
            "[ 318] loss =   0.12363847 \t 10.386s\n",
            "[ 319] loss =   0.12348162 \t 10.217s\n",
            "[ 320] loss =   0.12332568 \t 10.170s\n",
            "[ 321] loss =   0.12316850 \t 10.209s\n",
            "[ 322] loss =   0.12301249 \t 10.275s\n",
            "[ 323] loss =   0.12285546 \t 10.328s\n",
            "[ 324] loss =   0.12269903 \t 10.143s\n",
            "[ 325] loss =   0.12254217 \t 10.377s\n",
            "[ 326] loss =   0.12238554 \t 10.209s\n",
            "[ 327] loss =   0.12222888 \t 9.969s\n",
            "[ 328] loss =   0.12207257 \t 10.477s\n",
            "[ 329] loss =   0.12191501 \t 10.319s\n",
            "[ 330] loss =   0.12175834 \t 9.994s\n",
            "[ 331] loss =   0.12160160 \t 10.280s\n",
            "[ 332] loss =   0.12144493 \t 10.222s\n",
            "[ 333] loss =   0.12128811 \t 9.987s\n",
            "[ 334] loss =   0.12113129 \t 10.438s\n",
            "[ 335] loss =   0.12097440 \t 10.126s\n",
            "[ 336] loss =   0.12081660 \t 10.071s\n",
            "[ 337] loss =   0.12065978 \t 10.176s\n",
            "[ 338] loss =   0.12050296 \t 10.090s\n",
            "[ 339] loss =   0.12034609 \t 10.138s\n",
            "[ 340] loss =   0.12018844 \t 10.210s\n",
            "[ 341] loss =   0.12003130 \t 10.180s\n",
            "[ 342] loss =   0.11987372 \t 10.331s\n",
            "[ 343] loss =   0.11971760 \t 10.087s\n",
            "[ 344] loss =   0.11956041 \t 10.062s\n",
            "[ 345] loss =   0.11940312 \t 10.032s\n",
            "[ 346] loss =   0.11924575 \t 10.322s\n",
            "[ 347] loss =   0.11908900 \t 10.061s\n",
            "[ 348] loss =   0.11893080 \t 10.234s\n",
            "[ 349] loss =   0.11877345 \t 10.062s\n",
            "[ 350] loss =   0.11861661 \t 10.142s\n",
            "[ 351] loss =   0.11845966 \t 10.145s\n",
            "[ 352] loss =   0.11830204 \t 10.135s\n",
            "[ 353] loss =   0.11814430 \t 10.247s\n",
            "[ 354] loss =   0.11798694 \t 10.097s\n",
            "[ 355] loss =   0.11782995 \t 10.372s\n",
            "[ 356] loss =   0.11767327 \t 10.016s\n",
            "[ 357] loss =   0.11751594 \t 10.038s\n",
            "[ 358] loss =   0.11735750 \t 10.111s\n",
            "[ 359] loss =   0.11720041 \t 10.192s\n",
            "[ 360] loss =   0.11704307 \t 10.153s\n",
            "[ 361] loss =   0.11688569 \t 10.494s\n",
            "[ 362] loss =   0.11672890 \t 10.205s\n",
            "[ 363] loss =   0.11657117 \t 10.360s\n",
            "[ 364] loss =   0.11641334 \t 10.164s\n",
            "[ 365] loss =   0.11625656 \t 10.171s\n",
            "[ 366] loss =   0.11609883 \t 10.340s\n",
            "[ 367] loss =   0.11594161 \t 10.065s\n",
            "[ 368] loss =   0.11578418 \t 10.023s\n",
            "[ 369] loss =   0.11562660 \t 10.096s\n",
            "[ 370] loss =   0.11546942 \t 10.057s\n",
            "[ 371] loss =   0.11531192 \t 10.080s\n",
            "[ 372] loss =   0.11515434 \t 10.006s\n",
            "[ 373] loss =   0.11499713 \t 10.358s\n",
            "[ 374] loss =   0.11484020 \t 10.052s\n",
            "[ 375] loss =   0.11468238 \t 10.188s\n",
            "[ 376] loss =   0.11452565 \t 10.200s\n",
            "[ 377] loss =   0.11436792 \t 10.114s\n",
            "[ 378] loss =   0.11421025 \t 10.014s\n",
            "[ 379] loss =   0.11405264 \t 10.328s\n",
            "[ 380] loss =   0.11389537 \t 10.057s\n",
            "[ 381] loss =   0.11373810 \t 10.066s\n",
            "[ 382] loss =   0.11358107 \t 10.237s\n",
            "[ 383] loss =   0.11342368 \t 10.048s\n",
            "[ 384] loss =   0.11326660 \t 10.191s\n",
            "[ 385] loss =   0.11310870 \t 10.216s\n",
            "[ 386] loss =   0.11295116 \t 10.202s\n",
            "[ 387] loss =   0.11279410 \t 10.258s\n",
            "[ 388] loss =   0.11263669 \t 10.023s\n",
            "[ 389] loss =   0.11247989 \t 10.351s\n",
            "[ 390] loss =   0.11232208 \t 10.230s\n",
            "[ 391] loss =   0.11216573 \t 10.235s\n",
            "[ 392] loss =   0.11200703 \t 10.040s\n",
            "[ 393] loss =   0.11185095 \t 10.238s\n",
            "[ 394] loss =   0.11169334 \t 10.236s\n",
            "[ 395] loss =   0.11153626 \t 10.178s\n",
            "[ 396] loss =   0.11137921 \t 10.080s\n",
            "[ 397] loss =   0.11122148 \t 10.155s\n",
            "[ 398] loss =   0.11106562 \t 10.025s\n",
            "[ 399] loss =   0.11090771 \t 10.220s\n",
            "[ 400] loss =   0.11075067 \t 9.995s\n",
            "[ 401] loss =   0.11059371 \t 10.050s\n",
            "[ 402] loss =   0.11043670 \t 10.154s\n",
            "[ 403] loss =   0.11027997 \t 10.193s\n",
            "[ 404] loss =   0.11012226 \t 10.218s\n",
            "[ 405] loss =   0.10996567 \t 10.133s\n",
            "[ 406] loss =   0.10980830 \t 10.035s\n",
            "[ 407] loss =   0.10965122 \t 10.023s\n",
            "[ 408] loss =   0.10949507 \t 10.099s\n",
            "[ 409] loss =   0.10933790 \t 10.518s\n",
            "[ 410] loss =   0.10918084 \t 10.139s\n",
            "[ 411] loss =   0.10902354 \t 10.175s\n",
            "[ 412] loss =   0.10886762 \t 10.172s\n",
            "[ 413] loss =   0.10871010 \t 10.055s\n",
            "[ 414] loss =   0.10855298 \t 10.081s\n",
            "[ 415] loss =   0.10839649 \t 9.969s\n",
            "[ 416] loss =   0.10824043 \t 10.059s\n",
            "[ 417] loss =   0.10808406 \t 10.030s\n",
            "[ 418] loss =   0.10792734 \t 10.085s\n",
            "[ 419] loss =   0.10777039 \t 10.012s\n",
            "[ 420] loss =   0.10761406 \t 10.051s\n",
            "[ 421] loss =   0.10745741 \t 10.385s\n",
            "[ 422] loss =   0.10730081 \t 10.044s\n",
            "[ 423] loss =   0.10714431 \t 10.125s\n",
            "[ 424] loss =   0.10698821 \t 10.113s\n",
            "[ 425] loss =   0.10683139 \t 10.129s\n",
            "[ 426] loss =   0.10667528 \t 10.165s\n",
            "[ 427] loss =   0.10651894 \t 10.222s\n",
            "[ 428] loss =   0.10636264 \t 10.239s\n",
            "[ 429] loss =   0.10620641 \t 10.310s\n",
            "[ 430] loss =   0.10604992 \t 10.093s\n",
            "[ 431] loss =   0.10589343 \t 10.128s\n",
            "[ 432] loss =   0.10573693 \t 10.426s\n",
            "[ 433] loss =   0.10558125 \t 10.273s\n",
            "[ 434] loss =   0.10542537 \t 10.351s\n",
            "[ 435] loss =   0.10526948 \t 10.292s\n",
            "[ 436] loss =   0.10511323 \t 10.113s\n",
            "[ 437] loss =   0.10495798 \t 10.311s\n",
            "[ 438] loss =   0.10480153 \t 10.544s\n",
            "[ 439] loss =   0.10464504 \t 10.231s\n",
            "[ 440] loss =   0.10448990 \t 10.357s\n",
            "[ 441] loss =   0.10433435 \t 10.316s\n",
            "[ 442] loss =   0.10417853 \t 10.085s\n",
            "[ 443] loss =   0.10402222 \t 10.072s\n",
            "[ 444] loss =   0.10386702 \t 10.101s\n",
            "[ 445] loss =   0.10371166 \t 10.299s\n",
            "[ 446] loss =   0.10355555 \t 10.119s\n",
            "[ 447] loss =   0.10339981 \t 10.263s\n",
            "[ 448] loss =   0.10324515 \t 10.251s\n",
            "[ 449] loss =   0.10308990 \t 10.164s\n",
            "[ 450] loss =   0.10293401 \t 10.259s\n",
            "[ 451] loss =   0.10277896 \t 10.218s\n",
            "[ 452] loss =   0.10262351 \t 10.074s\n",
            "[ 453] loss =   0.10246804 \t 10.098s\n",
            "[ 454] loss =   0.10231289 \t 10.272s\n",
            "[ 455] loss =   0.10215905 \t 10.244s\n",
            "[ 456] loss =   0.10200294 \t 10.003s\n",
            "[ 457] loss =   0.10184747 \t 10.244s\n",
            "[ 458] loss =   0.10169230 \t 10.194s\n",
            "[ 459] loss =   0.10153756 \t 10.061s\n",
            "[ 460] loss =   0.10138297 \t 10.321s\n",
            "[ 461] loss =   0.10122816 \t 10.185s\n",
            "[ 462] loss =   0.10107353 \t 10.122s\n",
            "[ 463] loss =   0.10091867 \t 10.033s\n",
            "[ 464] loss =   0.10076424 \t 10.126s\n",
            "[ 465] loss =   0.10060941 \t 10.212s\n",
            "[ 466] loss =   0.10045502 \t 10.184s\n",
            "[ 467] loss =   0.10030039 \t 10.171s\n",
            "[ 468] loss =   0.10014618 \t 10.184s\n",
            "[ 469] loss =   0.09999197 \t 10.017s\n",
            "[ 470] loss =   0.09983796 \t 10.313s\n",
            "[ 471] loss =   0.09968318 \t 10.193s\n",
            "[ 472] loss =   0.09952865 \t 10.180s\n",
            "[ 473] loss =   0.09937486 \t 10.357s\n",
            "[ 474] loss =   0.09922107 \t 10.139s\n",
            "[ 475] loss =   0.09906729 \t 10.106s\n",
            "[ 476] loss =   0.09891292 \t 10.138s\n",
            "[ 477] loss =   0.09875937 \t 10.135s\n",
            "[ 478] loss =   0.09860522 \t 10.166s\n",
            "[ 479] loss =   0.09845155 \t 10.103s\n",
            "[ 480] loss =   0.09829722 \t 10.183s\n",
            "[ 481] loss =   0.09814432 \t 10.056s\n",
            "[ 482] loss =   0.09799068 \t 10.053s\n",
            "[ 483] loss =   0.09783706 \t 10.209s\n",
            "[ 484] loss =   0.09768386 \t 10.150s\n",
            "[ 485] loss =   0.09753033 \t 10.316s\n",
            "[ 486] loss =   0.09737726 \t 10.064s\n",
            "[ 487] loss =   0.09722391 \t 10.267s\n",
            "[ 488] loss =   0.09707040 \t 10.048s\n",
            "[ 489] loss =   0.09691765 \t 10.070s\n",
            "[ 490] loss =   0.09676427 \t 10.096s\n",
            "[ 491] loss =   0.09661128 \t 10.022s\n",
            "[ 492] loss =   0.09645857 \t 10.041s\n",
            "[ 493] loss =   0.09630594 \t 10.200s\n",
            "[ 494] loss =   0.09615308 \t 10.193s\n",
            "[ 495] loss =   0.09600117 \t 10.235s\n",
            "[ 496] loss =   0.09584809 \t 10.055s\n",
            "[ 497] loss =   0.09569490 \t 10.091s\n",
            "[ 498] loss =   0.09554230 \t 10.267s\n",
            "[ 499] loss =   0.09539021 \t 10.197s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(U, x, t, 0.001, 0.002, 500, 'l-bfgs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzwFRwBNdRHk",
        "outputId": "e2685b83-cfb7-44f6-f268-432149b6692b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =      0.18729 \t 7.933s\n",
            "loss =      0.18729 \t 7.099s\n",
            "loss =      0.18729 \t 7.634s\n",
            "loss =      0.18729 \t 7.158s\n",
            "loss =      0.18729 \t 7.229s\n",
            "loss =      0.18729 \t 7.407s\n",
            "loss =      0.18729 \t 8.627s\n",
            "loss =      0.18729 \t 7.270s\n",
            "loss =      0.18729 \t 7.522s\n",
            "loss =      0.18729 \t 7.235s\n",
            "loss =      0.18729 \t 7.180s\n",
            "loss =      0.18729 \t 7.193s\n",
            "loss =      0.18729 \t 7.438s\n",
            "loss =      0.18729 \t 7.653s\n",
            "loss =      0.18729 \t 7.363s\n",
            "loss =      0.18729 \t 7.877s\n",
            "loss =      0.18729 \t 7.690s\n",
            "loss =      0.18729 \t 7.198s\n",
            "loss =      0.18729 \t 7.399s\n",
            "loss =      0.18729 \t 7.178s\n",
            "loss =      0.18729 \t 7.311s\n",
            "loss =      0.18729 \t 7.432s\n",
            "loss =      0.18729 \t 7.041s\n",
            "loss =      0.18729 \t 7.292s\n",
            "loss =      0.18729 \t 7.699s\n",
            "loss =      0.18729 \t 7.117s\n",
            "loss =      0.18729 \t 7.564s\n",
            "loss =      0.18729 \t 7.029s\n",
            "loss =      0.18729 \t 7.174s\n",
            "loss =      0.18729 \t 7.454s\n",
            "loss =      0.18729 \t 7.017s\n",
            "loss =      0.18729 \t 7.224s\n",
            "loss =      0.18729 \t 7.332s\n",
            "loss =      0.18729 \t 6.982s\n",
            "loss =      0.18729 \t 7.204s\n",
            "loss =      0.18729 \t 7.353s\n",
            "loss =      0.18729 \t 6.983s\n",
            "loss =      0.18729 \t 7.453s\n",
            "loss =      0.18729 \t 7.474s\n",
            "loss =      0.18729 \t 7.333s\n",
            "loss =      0.18729 \t 7.020s\n",
            "loss =      0.18729 \t 7.189s\n",
            "loss =      0.18729 \t 7.281s\n",
            "loss =      0.18729 \t 7.047s\n",
            "loss =      0.18729 \t 7.198s\n",
            "loss =      0.18729 \t 7.376s\n",
            "loss =      0.18729 \t 7.086s\n",
            "loss =      0.18729 \t 7.161s\n",
            "loss =      0.18729 \t 7.334s\n",
            "loss =      0.18729 \t 7.271s\n",
            "loss =      0.18729 \t 7.446s\n",
            "loss =      0.18729 \t 7.095s\n",
            "loss =      0.18729 \t 7.184s\n",
            "loss =      0.18729 \t 7.328s\n",
            "loss =      0.18729 \t 7.128s\n",
            "loss =      0.18729 \t 7.321s\n",
            "loss =      0.18729 \t 7.262s\n",
            "loss =      0.18729 \t 7.362s\n",
            "loss =      0.18729 \t 6.983s\n",
            "loss =      0.18729 \t 7.189s\n",
            "loss =      0.18729 \t 7.218s\n",
            "loss =      0.18729 \t 7.170s\n",
            "loss =      0.18729 \t 7.206s\n",
            "loss =      0.18729 \t 7.281s\n",
            "loss =      0.18729 \t 7.269s\n",
            "loss =      0.18729 \t 7.183s\n",
            "loss =      0.18729 \t 7.332s\n",
            "loss =      0.18729 \t 7.168s\n",
            "loss =      0.18729 \t 7.257s\n",
            "loss =      0.18729 \t 7.271s\n",
            "loss =      0.18729 \t 7.157s\n",
            "loss =      0.18729 \t 7.153s\n",
            "loss =      0.18729 \t 7.380s\n",
            "loss =      0.18729 \t 7.124s\n",
            "loss =      0.18729 \t 7.174s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.reshape(tf.convert_to_tensor(0.7, dtype=tf.float32), [1])\n",
        "print(a)\n",
        "PDEmodel(a)[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WiZ-c-dmUkg",
        "outputId": "03caeb3f-1a26-4853-a1a8-a2ad0d9c4355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.7], shape=(1,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.9766004>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def c_exact(x):\n",
        "    return 1+np.exp(-(x-0.5)**2)"
      ],
      "metadata": {
        "id": "TRgr0rOknEUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_exact(0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4PM4N5Mq7hX",
        "outputId": "194e881f-f79d-4465-b8b2-eceb1dc500d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8521437889662113"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# line 1 points\n",
        "x1 = x\n",
        "y1 = []\n",
        "for el in x:\n",
        "    a = tf.reshape(tf.convert_to_tensor(el, dtype=tf.float32), [1])\n",
        "    y1.append(PDEmodel(a)[0][0])\n",
        "# plotting the line 1 points\n",
        "plt.plot(x1, y1, label = \"predict\")\n",
        " \n",
        "# line 2 points\n",
        "x2 = x\n",
        "y2 = []\n",
        "for el in x:\n",
        "    y2.append(c_exact(el))\n",
        "# plotting the line 2 points\n",
        "plt.plot(x2, y2, label = \"exact\")\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('x - axis')\n",
        "# naming the y axis\n",
        "plt.ylabel('c - axis')\n",
        "# giving a title to my graph\n",
        "plt.title('c(x) approximate')\n",
        " \n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "6wwPY0x_LaGQ",
        "outputId": "a659cea6-e4b7-4f7c-a84f-08eddeaf23b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}